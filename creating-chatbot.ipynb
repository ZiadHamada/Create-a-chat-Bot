{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom string import punctuation\npunctuation = list(punctuation)\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-02T21:00:31.153758Z","iopub.execute_input":"2023-08-02T21:00:31.154125Z","iopub.status.idle":"2023-08-02T21:00:32.335614Z","shell.execute_reply.started":"2023-08-02T21:00:31.154093Z","shell.execute_reply":"2023-08-02T21:00:32.334606Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/cleaned-data-for-the-chatbot-collected-from-movies/dialogs_expanded.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:00:32.337832Z","iopub.execute_input":"2023-08-02T21:00:32.338210Z","iopub.status.idle":"2023-08-02T21:00:33.738390Z","shell.execute_reply.started":"2023-08-02T21:00:32.338174Z","shell.execute_reply":"2023-08-02T21:00:33.737255Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"        Unnamed: 0                                           question  \\\n0                1  Well, I thought we'd start with pronunciation,...   \n1                2  Not the hacking and gagging and spitting part....   \n2                3  You're asking me out.  That's so cute. What's ...   \n3                4  No, no, it's my fault -- we didn't have a prop...   \n4                9     Gosh, if only we could find Kat a boyfriend...   \n...            ...                                                ...   \n139404      221608    Well that one. The one who keeps looking at me.   \n139405      221609  Choose your targets men. That's right Watch th...   \n139406      221610  Colonel Durnford... William Vereker. I hear yo...   \n139407      221611                           Your orders, Mr Vereker?   \n139408      221612  I'm to take the Sikali with the main column to...   \n\n                                                   answer  \\\n0       Not the hacking and gagging and spitting part....   \n1       Okay... then how 'bout we try out some French ...   \n2                                              Forget it.   \n3                                                Cameron.   \n4                               Let me see what I can do.   \n...                                                   ...   \n139404  ft could be you flatter yourself CoghilL It's ...   \n139405  Keep steady. You're the best shots of the Twen...   \n139406  Good ones, yes, Mr Vereker. Gentlemen who can ...   \n139407  I'm to take the Sikali with the main column to...   \n139408  Lord Chelmsford seems to want me to stay back ...   \n\n                                          question_as_int  \\\n0       [54, 67, 74, 74, 12, 1, 40, 1, 82, 70, 77, 83,...   \n1       [45, 77, 82, 1, 82, 70, 67, 1, 70, 63, 65, 73,...   \n2       [56, 77, 83, 8, 80, 67, 1, 63, 81, 73, 71, 76,...   \n3       [45, 77, 12, 1, 76, 77, 12, 1, 71, 82, 8, 81, ...   \n4       [38, 77, 81, 70, 12, 1, 71, 68, 1, 77, 76, 74,...   \n...                                                   ...   \n139404  [54, 67, 74, 74, 1, 82, 70, 63, 82, 1, 77, 76,...   \n139405  [34, 70, 77, 77, 81, 67, 1, 87, 77, 83, 80, 1,...   \n139406  [34, 77, 74, 77, 76, 67, 74, 1, 35, 83, 80, 76...   \n139407  [56, 77, 83, 80, 1, 77, 80, 66, 67, 80, 81, 12...   \n139408  [40, 8, 75, 1, 82, 77, 1, 82, 63, 73, 67, 1, 8...   \n\n                                            answer_as_int  question_len  \\\n0       [45, 77, 82, 1, 82, 70, 67, 1, 70, 63, 65, 73,...            71   \n1       [46, 73, 63, 87, 14, 14, 14, 1, 82, 70, 67, 76...            55   \n2                 [37, 77, 80, 69, 67, 82, 1, 71, 82, 14]            62   \n3                        [34, 63, 75, 67, 80, 77, 76, 14]            65   \n4       [43, 67, 82, 1, 75, 67, 1, 81, 67, 67, 1, 85, ...            46   \n...                                                   ...           ...   \n139404  [68, 82, 1, 65, 77, 83, 74, 66, 1, 64, 67, 1, ...            47   \n139405  [42, 67, 67, 78, 1, 81, 82, 67, 63, 66, 87, 14...            61   \n139406  [38, 77, 77, 66, 1, 77, 76, 67, 81, 12, 1, 87,...            74   \n139407  [40, 8, 75, 1, 82, 77, 1, 82, 63, 73, 67, 1, 8...            24   \n139408  [43, 77, 80, 66, 1, 34, 70, 67, 74, 75, 81, 68...            56   \n\n        answer_len  \n0               55  \n1               73  \n2               10  \n3                8  \n4               25  \n...            ...  \n139404          59  \n139405          85  \n139406          60  \n139407          56  \n139408          62  \n\n[139409 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>question_as_int</th>\n      <th>answer_as_int</th>\n      <th>question_len</th>\n      <th>answer_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Well, I thought we'd start with pronunciation,...</td>\n      <td>Not the hacking and gagging and spitting part....</td>\n      <td>[54, 67, 74, 74, 12, 1, 40, 1, 82, 70, 77, 83,...</td>\n      <td>[45, 77, 82, 1, 82, 70, 67, 1, 70, 63, 65, 73,...</td>\n      <td>71</td>\n      <td>55</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Not the hacking and gagging and spitting part....</td>\n      <td>Okay... then how 'bout we try out some French ...</td>\n      <td>[45, 77, 82, 1, 82, 70, 67, 1, 70, 63, 65, 73,...</td>\n      <td>[46, 73, 63, 87, 14, 14, 14, 1, 82, 70, 67, 76...</td>\n      <td>55</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>You're asking me out.  That's so cute. What's ...</td>\n      <td>Forget it.</td>\n      <td>[56, 77, 83, 8, 80, 67, 1, 63, 81, 73, 71, 76,...</td>\n      <td>[37, 77, 80, 69, 67, 82, 1, 71, 82, 14]</td>\n      <td>62</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>No, no, it's my fault -- we didn't have a prop...</td>\n      <td>Cameron.</td>\n      <td>[45, 77, 12, 1, 76, 77, 12, 1, 71, 82, 8, 81, ...</td>\n      <td>[34, 63, 75, 67, 80, 77, 76, 14]</td>\n      <td>65</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>Gosh, if only we could find Kat a boyfriend...</td>\n      <td>Let me see what I can do.</td>\n      <td>[38, 77, 81, 70, 12, 1, 71, 68, 1, 77, 76, 74,...</td>\n      <td>[43, 67, 82, 1, 75, 67, 1, 81, 67, 67, 1, 85, ...</td>\n      <td>46</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>139404</th>\n      <td>221608</td>\n      <td>Well that one. The one who keeps looking at me.</td>\n      <td>ft could be you flatter yourself CoghilL It's ...</td>\n      <td>[54, 67, 74, 74, 1, 82, 70, 63, 82, 1, 77, 76,...</td>\n      <td>[68, 82, 1, 65, 77, 83, 74, 66, 1, 64, 67, 1, ...</td>\n      <td>47</td>\n      <td>59</td>\n    </tr>\n    <tr>\n      <th>139405</th>\n      <td>221609</td>\n      <td>Choose your targets men. That's right Watch th...</td>\n      <td>Keep steady. You're the best shots of the Twen...</td>\n      <td>[34, 70, 77, 77, 81, 67, 1, 87, 77, 83, 80, 1,...</td>\n      <td>[42, 67, 67, 78, 1, 81, 82, 67, 63, 66, 87, 14...</td>\n      <td>61</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>139406</th>\n      <td>221610</td>\n      <td>Colonel Durnford... William Vereker. I hear yo...</td>\n      <td>Good ones, yes, Mr Vereker. Gentlemen who can ...</td>\n      <td>[34, 77, 74, 77, 76, 67, 74, 1, 35, 83, 80, 76...</td>\n      <td>[38, 77, 77, 66, 1, 77, 76, 67, 81, 12, 1, 87,...</td>\n      <td>74</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>139407</th>\n      <td>221611</td>\n      <td>Your orders, Mr Vereker?</td>\n      <td>I'm to take the Sikali with the main column to...</td>\n      <td>[56, 77, 83, 80, 1, 77, 80, 66, 67, 80, 81, 12...</td>\n      <td>[40, 8, 75, 1, 82, 77, 1, 82, 63, 73, 67, 1, 8...</td>\n      <td>24</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>139408</th>\n      <td>221612</td>\n      <td>I'm to take the Sikali with the main column to...</td>\n      <td>Lord Chelmsford seems to want me to stay back ...</td>\n      <td>[40, 8, 75, 1, 82, 77, 1, 82, 63, 73, 67, 1, 8...</td>\n      <td>[43, 77, 80, 66, 1, 34, 70, 67, 74, 75, 81, 68...</td>\n      <td>56</td>\n      <td>62</td>\n    </tr>\n  </tbody>\n</table>\n<p>139409 rows Ã— 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:00:33.739753Z","iopub.execute_input":"2023-08-02T21:00:33.740203Z","iopub.status.idle":"2023-08-02T21:00:33.917642Z","shell.execute_reply.started":"2023-08-02T21:00:33.740165Z","shell.execute_reply":"2023-08-02T21:00:33.916465Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Unnamed: 0         0\nquestion           0\nanswer             0\nquestion_as_int    0\nanswer_as_int      0\nquestion_len       0\nanswer_len         0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# Modify the form of some words\nreplace_list = {r\"'m\": ' am',\n                r\"'re\": ' are',\n                r\"letâ€™s\": 'let us',\n                r\"'s\":  ' is',\n                r\"'ve\": ' have',\n                r\"can't\": 'can not',\n                r\"cannot\": 'can not',\n                r\"shanâ€™t\": 'shall not',\n                r\"n't\": ' not',\n                r\"'d\": ' would',\n                r\"'ll\": ' will',\n                r\"'scuse\": 'excuse',\n                ',': ' ,',\n                '.': ' .',\n                '!': ' !',\n                '?': ' ?',\n                '\\s+': ' '}\ndef clean_text(text):\n    text = text.lower()\n    for s in replace_list:\n        text = text.replace(s, replace_list[s])\n    text = ' '.join(text.split())\n    return text\n# Apply the clean_text function\ndata['question'] = data['question'].apply(lambda p: clean_text(p))\ndata['answer'] = data['answer'].apply(lambda p: clean_text(p))","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:00:33.921563Z","iopub.execute_input":"2023-08-02T21:00:33.921858Z","iopub.status.idle":"2023-08-02T21:00:35.657869Z","shell.execute_reply.started":"2023-08-02T21:00:33.921831Z","shell.execute_reply":"2023-08-02T21:00:35.656826Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_ques = data['question']\ndata_ans = data['answer']","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:00:35.659513Z","iopub.execute_input":"2023-08-02T21:00:35.659894Z","iopub.status.idle":"2023-08-02T21:00:35.667111Z","shell.execute_reply.started":"2023-08-02T21:00:35.659857Z","shell.execute_reply":"2023-08-02T21:00:35.666003Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:00:35.668701Z","iopub.execute_input":"2023-08-02T21:00:35.669115Z","iopub.status.idle":"2023-08-02T21:00:36.972427Z","shell.execute_reply.started":"2023-08-02T21:00:35.669082Z","shell.execute_reply":"2023-08-02T21:00:36.971217Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocessing_data(data):\n    #tokenization\n    data_tokens = [word_tokenize(data[i]) for i in range(len(data))]\n    \n    #remove stopwords\n    data_cleaned = []\n    for i in range(len(data_tokens)):\n        data_cleaned.append(' '.join([lemmatizer.lemmatize(word.lower(), pos=\"v\") for word in data_tokens[i] \n                             if not word.lower() in stop_words and word not in punctuation]))  \n    return data_cleaned","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:00:36.974564Z","iopub.execute_input":"2023-08-02T21:00:36.974946Z","iopub.status.idle":"2023-08-02T21:00:36.984082Z","shell.execute_reply.started":"2023-08-02T21:00:36.974910Z","shell.execute_reply":"2023-08-02T21:00:36.982463Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data_ques = preprocessing_data(data_ques)\ndata_ans = preprocessing_data(data_ans)\n\ndata_ques = np.array(data_ques)\ndata_ans = np.array(data_ans)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:00:36.985773Z","iopub.execute_input":"2023-08-02T21:00:36.986186Z","iopub.status.idle":"2023-08-02T21:01:50.645935Z","shell.execute_reply.started":"2023-08-02T21:00:36.986151Z","shell.execute_reply":"2023-08-02T21:01:50.644759Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"zip_data =  list(zip(data_ques, data_ans))\nlines = pd.DataFrame(zip_data, columns = ['questions' , 'answers']) \nlines.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:01:50.647403Z","iopub.execute_input":"2023-08-02T21:01:50.647840Z","iopub.status.idle":"2023-08-02T21:01:50.894237Z","shell.execute_reply.started":"2023-08-02T21:01:50.647801Z","shell.execute_reply":"2023-08-02T21:01:50.893260Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                   questions  \\\n0  well think would start pronunciation okay   \n1                  hack gag spit part please   \n2                              ask cute name   \n3            fault -- proper introduction --   \n4              gosh could find kat boyfriend   \n\n                                        answers  \n0                     hack gag spit part please  \n1  okay 'bout try french cuisine saturday night  \n2                                        forget  \n3                                       cameron  \n4                                       let see  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>questions</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>well think would start pronunciation okay</td>\n      <td>hack gag spit part please</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hack gag spit part please</td>\n      <td>okay 'bout try french cuisine saturday night</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ask cute name</td>\n      <td>forget</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fault -- proper introduction --</td>\n      <td>cameron</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>gosh could find kat boyfriend</td>\n      <td>let see</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:01:50.898068Z","iopub.execute_input":"2023-08-02T21:01:50.898398Z","iopub.status.idle":"2023-08-02T21:02:00.036665Z","shell.execute_reply.started":"2023-08-02T21:01:50.898370Z","shell.execute_reply":"2023-08-02T21:02:00.035617Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"questions = list()\nfor line in lines.questions:\n    questions.append(line) \n    \ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(questions) \ntokenized_questions = tokenizer.texts_to_sequences(questions) \n\nlength_list = list()\nfor token_seq in tokenized_questions:\n    length_list.append(len(token_seq))\nmax_question_length = np.array(length_list).max()\nprint( 'Input max length is {}'.format(max_question_length))\n\npadded_questions = pad_sequences(tokenized_questions ,maxlen=max_question_length, padding='post' )\nencoder_questions_data = np.array(padded_questions)\nprint( 'Encoder input data shape -> {}'.format( encoder_questions_data.shape ))\n\nquestions_word_dict = tokenizer.word_index\nnum_question_tokens = len( questions_word_dict )+1\nprint( 'Number of Input tokens = {}'.format(num_question_tokens))","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:02:00.038076Z","iopub.execute_input":"2023-08-02T21:02:00.038861Z","iopub.status.idle":"2023-08-02T21:02:04.545505Z","shell.execute_reply.started":"2023-08-02T21:02:00.038824Z","shell.execute_reply":"2023-08-02T21:02:04.544502Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Input max length is 20\nEncoder input data shape -> (139409, 20)\nNumber of Input tokens = 25285\n","output_type":"stream"}]},{"cell_type":"code","source":"answers = list()\nfor line in lines.answers:\n    answers.append('<START> ' + line + ' <END>')  \n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts( answers ) \ntokenized_answers = tokenizer.texts_to_sequences( answers ) \n\nlength_list = list()\nfor token_seq in tokenized_answers:\n    length_list.append( len( token_seq ))\nmax_answer_length = np.array( length_list ).max()\nprint( 'Output max length is {}'.format( max_answer_length ))\n\npadded_answers = pad_sequences( tokenized_answers , maxlen=max_answer_length, padding='post' )\ndecoder_answers_data = np.array( padded_answers )\nprint( 'Decoder input data shape -> {}'.format( decoder_answers_data.shape ))\n\nanswers_word_dict = tokenizer.word_index\nnum_answer_tokens = len( answers_word_dict )+1\nprint( 'Number of Output tokens = {}'.format( num_answer_tokens))","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:02:04.546861Z","iopub.execute_input":"2023-08-02T21:02:04.548810Z","iopub.status.idle":"2023-08-02T21:02:08.931961Z","shell.execute_reply.started":"2023-08-02T21:02:04.548771Z","shell.execute_reply":"2023-08-02T21:02:08.930990Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Output max length is 25\nDecoder input data shape -> (139409, 25)\nNumber of Output tokens = 24993\n","output_type":"stream"}]},{"cell_type":"code","source":"decoder_target_data = list()\nfor token_seq in tokenized_answers:\n    decoder_target_data.append(token_seq[1: ]) \n    \npadded_answers_lines = pad_sequences(decoder_target_data, maxlen=max_answer_length, padding='post')\ndecoder_target_data = np.array(padded_answers_lines)\nprint( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:02:08.933625Z","iopub.execute_input":"2023-08-02T21:02:08.934646Z","iopub.status.idle":"2023-08-02T21:02:09.896956Z","shell.execute_reply.started":"2023-08-02T21:02:08.934604Z","shell.execute_reply":"2023-08-02T21:02:09.895782Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Decoder target data shape -> (139409, 25)\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nencoder_inputs = tf.keras.layers.Input(shape=( None , ))\nencoder_embedding = tf.keras.layers.Embedding( num_question_tokens, 128 , mask_zero=True ) (encoder_inputs)\nencoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 128 , return_state=True , recurrent_dropout=0.3 , \n                                                           dropout=0.5 )( encoder_embedding )\nencoder_states = [ state_h , state_c ]\n\ndecoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\ndecoder_embedding = tf.keras.layers.Embedding( num_answer_tokens, 128 , mask_zero=True) (decoder_inputs)\ndecoder_lstm = tf.keras.layers.LSTM( 128 , return_state=True , return_sequences=True , recurrent_dropout=0.3 , \n                                    dropout=0.5)\ndecoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\ndecoder_dense = tf.keras.layers.Dense( num_answer_tokens , activation=tf.keras.activations.softmax ) \noutput = decoder_dense ( decoder_outputs )\n\nmodel = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:02:09.898405Z","iopub.execute_input":"2023-08-02T21:02:09.899458Z","iopub.status.idle":"2023-08-02T21:02:14.770364Z","shell.execute_reply.started":"2023-08-02T21:02:09.899419Z","shell.execute_reply":"2023-08-02T21:02:14.769619Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, None)]       0           []                               \n                                                                                                  \n embedding (Embedding)          (None, None, 128)    3236480     ['input_1[0][0]']                \n                                                                                                  \n embedding_1 (Embedding)        (None, None, 128)    3199104     ['input_2[0][0]']                \n                                                                                                  \n lstm (LSTM)                    [(None, 128),        131584      ['embedding[0][0]']              \n                                 (None, 128),                                                     \n                                 (None, 128)]                                                     \n                                                                                                  \n lstm_1 (LSTM)                  [(None, None, 128),  131584      ['embedding_1[0][0]',            \n                                 (None, 128),                     'lstm[0][1]',                   \n                                 (None, 128)]                     'lstm[0][2]']                   \n                                                                                                  \n dense (Dense)                  (None, None, 24993)  3224097     ['lstm_1[0][0]']                 \n                                                                                                  \n==================================================================================================\nTotal params: 9,922,849\nTrainable params: 9,922,849\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit([encoder_questions_data , decoder_answers_data], decoder_target_data,\n          batch_size=512, epochs=200, verbose=1) ","metadata":{"execution":{"iopub.status.busy":"2023-08-02T21:02:14.771481Z","iopub.execute_input":"2023-08-02T21:02:14.772073Z","iopub.status.idle":"2023-08-03T03:48:42.966320Z","shell.execute_reply.started":"2023-08-02T21:02:14.772033Z","shell.execute_reply":"2023-08-03T03:48:42.965250Z"},"scrolled":true,"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/200\n273/273 [==============================] - 186s 639ms/step - loss: 6.4881 - accuracy: 0.2080\nEpoch 2/200\n273/273 [==============================] - 132s 481ms/step - loss: 5.3860 - accuracy: 0.3051\nEpoch 3/200\n273/273 [==============================] - 128s 470ms/step - loss: 5.1075 - accuracy: 0.3626\nEpoch 4/200\n273/273 [==============================] - 125s 456ms/step - loss: 5.0576 - accuracy: 0.3632\nEpoch 5/200\n273/273 [==============================] - 125s 457ms/step - loss: 5.0253 - accuracy: 0.3639\nEpoch 6/200\n273/273 [==============================] - 125s 457ms/step - loss: 4.9989 - accuracy: 0.3644\nEpoch 7/200\n273/273 [==============================] - 125s 457ms/step - loss: 4.9773 - accuracy: 0.3647\nEpoch 8/200\n273/273 [==============================] - 124s 452ms/step - loss: 4.9589 - accuracy: 0.3666\nEpoch 9/200\n273/273 [==============================] - 124s 452ms/step - loss: 4.9427 - accuracy: 0.3670\nEpoch 10/200\n273/273 [==============================] - 124s 454ms/step - loss: 4.9270 - accuracy: 0.3674\nEpoch 11/200\n273/273 [==============================] - 123s 451ms/step - loss: 4.9104 - accuracy: 0.3677\nEpoch 12/200\n273/273 [==============================] - 124s 454ms/step - loss: 4.8936 - accuracy: 0.3678\nEpoch 13/200\n273/273 [==============================] - 123s 452ms/step - loss: 4.8768 - accuracy: 0.3679\nEpoch 14/200\n273/273 [==============================] - 124s 453ms/step - loss: 4.8614 - accuracy: 0.3680\nEpoch 15/200\n273/273 [==============================] - 123s 452ms/step - loss: 4.8467 - accuracy: 0.3681\nEpoch 16/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.8324 - accuracy: 0.3684\nEpoch 17/200\n273/273 [==============================] - 123s 451ms/step - loss: 4.8180 - accuracy: 0.3685\nEpoch 18/200\n273/273 [==============================] - 123s 452ms/step - loss: 4.8039 - accuracy: 0.3688\nEpoch 19/200\n273/273 [==============================] - 122s 449ms/step - loss: 4.7895 - accuracy: 0.3691\nEpoch 20/200\n273/273 [==============================] - 123s 451ms/step - loss: 4.7755 - accuracy: 0.3695\nEpoch 21/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.7611 - accuracy: 0.3697\nEpoch 22/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.7471 - accuracy: 0.3700\nEpoch 23/200\n273/273 [==============================] - 123s 449ms/step - loss: 4.7331 - accuracy: 0.3703\nEpoch 24/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.7192 - accuracy: 0.3705\nEpoch 25/200\n273/273 [==============================] - 123s 450ms/step - loss: 4.7055 - accuracy: 0.3708\nEpoch 26/200\n273/273 [==============================] - 123s 452ms/step - loss: 4.6915 - accuracy: 0.3711\nEpoch 27/200\n273/273 [==============================] - 124s 454ms/step - loss: 4.6775 - accuracy: 0.3713\nEpoch 28/200\n273/273 [==============================] - 123s 452ms/step - loss: 4.6638 - accuracy: 0.3715\nEpoch 29/200\n273/273 [==============================] - 123s 450ms/step - loss: 4.6500 - accuracy: 0.3718\nEpoch 30/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.6364 - accuracy: 0.3721\nEpoch 31/200\n273/273 [==============================] - 122s 449ms/step - loss: 4.6226 - accuracy: 0.3724\nEpoch 32/200\n273/273 [==============================] - 123s 449ms/step - loss: 4.6093 - accuracy: 0.3727\nEpoch 33/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.5957 - accuracy: 0.3729\nEpoch 34/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.5823 - accuracy: 0.3733\nEpoch 35/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.5695 - accuracy: 0.3735\nEpoch 36/200\n273/273 [==============================] - 123s 450ms/step - loss: 4.5563 - accuracy: 0.3740\nEpoch 37/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.5427 - accuracy: 0.3743\nEpoch 38/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.5294 - accuracy: 0.3746\nEpoch 39/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.5162 - accuracy: 0.3748\nEpoch 40/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.5030 - accuracy: 0.3752\nEpoch 41/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.4902 - accuracy: 0.3754\nEpoch 42/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.4772 - accuracy: 0.3758\nEpoch 43/200\n273/273 [==============================] - 123s 449ms/step - loss: 4.4644 - accuracy: 0.3761\nEpoch 44/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.4518 - accuracy: 0.3767\nEpoch 45/200\n273/273 [==============================] - 123s 449ms/step - loss: 4.4386 - accuracy: 0.3770\nEpoch 46/200\n273/273 [==============================] - 122s 445ms/step - loss: 4.4264 - accuracy: 0.3774\nEpoch 47/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.4137 - accuracy: 0.3778\nEpoch 48/200\n273/273 [==============================] - 122s 445ms/step - loss: 4.4017 - accuracy: 0.3782\nEpoch 49/200\n273/273 [==============================] - 122s 445ms/step - loss: 4.3894 - accuracy: 0.3786\nEpoch 50/200\n273/273 [==============================] - 123s 450ms/step - loss: 4.3780 - accuracy: 0.3791\nEpoch 51/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.3652 - accuracy: 0.3795\nEpoch 52/200\n273/273 [==============================] - 123s 449ms/step - loss: 4.3540 - accuracy: 0.3799\nEpoch 53/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.3424 - accuracy: 0.3802\nEpoch 54/200\n273/273 [==============================] - 123s 450ms/step - loss: 4.3306 - accuracy: 0.3810\nEpoch 55/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.3184 - accuracy: 0.3811\nEpoch 56/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.3073 - accuracy: 0.3818\nEpoch 57/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.2963 - accuracy: 0.3822\nEpoch 58/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.2845 - accuracy: 0.3826\nEpoch 59/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.2741 - accuracy: 0.3831\nEpoch 60/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.2628 - accuracy: 0.3837\nEpoch 61/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.2514 - accuracy: 0.3842\nEpoch 62/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.2406 - accuracy: 0.3846\nEpoch 63/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.2294 - accuracy: 0.3851\nEpoch 64/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.2193 - accuracy: 0.3860\nEpoch 65/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.2091 - accuracy: 0.3865\nEpoch 66/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.1979 - accuracy: 0.3869\nEpoch 67/200\n273/273 [==============================] - 121s 444ms/step - loss: 4.1875 - accuracy: 0.3876\nEpoch 68/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.1769 - accuracy: 0.3881\nEpoch 69/200\n273/273 [==============================] - 121s 445ms/step - loss: 4.1672 - accuracy: 0.3888\nEpoch 70/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.1565 - accuracy: 0.3893\nEpoch 71/200\n273/273 [==============================] - 122s 445ms/step - loss: 4.1463 - accuracy: 0.3899\nEpoch 72/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.1369 - accuracy: 0.3907\nEpoch 73/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.1265 - accuracy: 0.3913\nEpoch 74/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.1175 - accuracy: 0.3918\nEpoch 75/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.1075 - accuracy: 0.3925\nEpoch 76/200\n273/273 [==============================] - 122s 446ms/step - loss: 4.0972 - accuracy: 0.3928\nEpoch 77/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.0875 - accuracy: 0.3935\nEpoch 78/200\n273/273 [==============================] - 121s 442ms/step - loss: 4.0791 - accuracy: 0.3940\nEpoch 79/200\n273/273 [==============================] - 121s 444ms/step - loss: 4.0691 - accuracy: 0.3947\nEpoch 80/200\n273/273 [==============================] - 121s 445ms/step - loss: 4.0601 - accuracy: 0.3955\nEpoch 81/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.0501 - accuracy: 0.3960\nEpoch 82/200\n273/273 [==============================] - 123s 449ms/step - loss: 4.0416 - accuracy: 0.3965\nEpoch 83/200\n273/273 [==============================] - 122s 448ms/step - loss: 4.0325 - accuracy: 0.3971\nEpoch 84/200\n273/273 [==============================] - 121s 444ms/step - loss: 4.0233 - accuracy: 0.3978\nEpoch 85/200\n273/273 [==============================] - 123s 449ms/step - loss: 4.0142 - accuracy: 0.3984\nEpoch 86/200\n273/273 [==============================] - 122s 447ms/step - loss: 4.0061 - accuracy: 0.3989\nEpoch 87/200\n273/273 [==============================] - 122s 448ms/step - loss: 3.9966 - accuracy: 0.3997\nEpoch 88/200\n273/273 [==============================] - 122s 448ms/step - loss: 3.9880 - accuracy: 0.4002\nEpoch 89/200\n273/273 [==============================] - 122s 446ms/step - loss: 3.9798 - accuracy: 0.4008\nEpoch 90/200\n273/273 [==============================] - 121s 445ms/step - loss: 3.9710 - accuracy: 0.4015\nEpoch 91/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.9624 - accuracy: 0.4023\nEpoch 92/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.9540 - accuracy: 0.4025\nEpoch 93/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.9462 - accuracy: 0.4034\nEpoch 94/200\n273/273 [==============================] - 121s 445ms/step - loss: 3.9371 - accuracy: 0.4040\nEpoch 95/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.9296 - accuracy: 0.4044\nEpoch 96/200\n273/273 [==============================] - 121s 441ms/step - loss: 3.9224 - accuracy: 0.4052\nEpoch 97/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.9144 - accuracy: 0.4056\nEpoch 98/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.9060 - accuracy: 0.4062\nEpoch 99/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.8983 - accuracy: 0.4070\nEpoch 100/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.8894 - accuracy: 0.4074\nEpoch 101/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.8824 - accuracy: 0.4080\nEpoch 102/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.8742 - accuracy: 0.4089\nEpoch 103/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.8664 - accuracy: 0.4091\nEpoch 104/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.8593 - accuracy: 0.4099\nEpoch 105/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.8514 - accuracy: 0.4107\nEpoch 106/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.8444 - accuracy: 0.4109\nEpoch 107/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.8366 - accuracy: 0.4118\nEpoch 108/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.8294 - accuracy: 0.4122\nEpoch 109/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.8217 - accuracy: 0.4129\nEpoch 110/200\n273/273 [==============================] - 121s 445ms/step - loss: 3.8151 - accuracy: 0.4133\nEpoch 111/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.8081 - accuracy: 0.4139\nEpoch 112/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.8013 - accuracy: 0.4145\nEpoch 113/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.7952 - accuracy: 0.4148\nEpoch 114/200\n273/273 [==============================] - 121s 445ms/step - loss: 3.7870 - accuracy: 0.4155\nEpoch 115/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.7808 - accuracy: 0.4161\nEpoch 116/200\n273/273 [==============================] - 121s 445ms/step - loss: 3.7730 - accuracy: 0.4166\nEpoch 117/200\n273/273 [==============================] - 122s 449ms/step - loss: 3.7671 - accuracy: 0.4173\nEpoch 118/200\n273/273 [==============================] - 122s 446ms/step - loss: 3.7602 - accuracy: 0.4177\nEpoch 119/200\n273/273 [==============================] - 122s 446ms/step - loss: 3.7535 - accuracy: 0.4184\nEpoch 120/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.7473 - accuracy: 0.4188\nEpoch 121/200\n273/273 [==============================] - 121s 441ms/step - loss: 3.7406 - accuracy: 0.4191\nEpoch 122/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.7338 - accuracy: 0.4199\nEpoch 123/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.7273 - accuracy: 0.4206\nEpoch 124/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.7211 - accuracy: 0.4206\nEpoch 125/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.7153 - accuracy: 0.4218\nEpoch 126/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.7086 - accuracy: 0.4222\nEpoch 127/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.7025 - accuracy: 0.4225\nEpoch 128/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.6902 - accuracy: 0.4237\nEpoch 130/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.6837 - accuracy: 0.4243\nEpoch 131/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.6778 - accuracy: 0.4249\nEpoch 132/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.6718 - accuracy: 0.4253\nEpoch 133/200\n273/273 [==============================] - 121s 441ms/step - loss: 3.6658 - accuracy: 0.4259\nEpoch 134/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.6609 - accuracy: 0.4264\nEpoch 135/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.6545 - accuracy: 0.4269\nEpoch 136/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.6477 - accuracy: 0.4273\nEpoch 137/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.6424 - accuracy: 0.4279\nEpoch 138/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.6376 - accuracy: 0.4284\nEpoch 139/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.6302 - accuracy: 0.4286\nEpoch 140/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.6253 - accuracy: 0.4293\nEpoch 141/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.6190 - accuracy: 0.4301\nEpoch 142/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.6139 - accuracy: 0.4303\nEpoch 143/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.6096 - accuracy: 0.4308\nEpoch 144/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.6043 - accuracy: 0.4312\nEpoch 145/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.5980 - accuracy: 0.4319\nEpoch 146/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.5919 - accuracy: 0.4323\nEpoch 147/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.5872 - accuracy: 0.4329\nEpoch 148/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.5816 - accuracy: 0.4334\nEpoch 149/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.5768 - accuracy: 0.4335\nEpoch 150/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.5715 - accuracy: 0.4343\nEpoch 151/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.5671 - accuracy: 0.4344\nEpoch 152/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.5602 - accuracy: 0.4352\nEpoch 153/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.5555 - accuracy: 0.4356\nEpoch 154/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.5509 - accuracy: 0.4360\nEpoch 155/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.5463 - accuracy: 0.4366\nEpoch 156/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.5418 - accuracy: 0.4369\nEpoch 157/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.5358 - accuracy: 0.4376\nEpoch 158/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.5296 - accuracy: 0.4379\nEpoch 159/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.5260 - accuracy: 0.4383\nEpoch 160/200\n273/273 [==============================] - 121s 441ms/step - loss: 3.5213 - accuracy: 0.4390\nEpoch 161/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.5163 - accuracy: 0.4393\nEpoch 162/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.5118 - accuracy: 0.4395\nEpoch 163/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.5072 - accuracy: 0.4399\nEpoch 164/200\n273/273 [==============================] - 122s 447ms/step - loss: 3.5026 - accuracy: 0.4402\nEpoch 165/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.4981 - accuracy: 0.4411\nEpoch 166/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.4926 - accuracy: 0.4412\nEpoch 167/200\n273/273 [==============================] - 121s 441ms/step - loss: 3.4892 - accuracy: 0.4416\nEpoch 168/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.4846 - accuracy: 0.4422\nEpoch 169/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.4782 - accuracy: 0.4428\nEpoch 170/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.4750 - accuracy: 0.4431\nEpoch 171/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.4700 - accuracy: 0.4437\nEpoch 172/200\n273/273 [==============================] - 120s 440ms/step - loss: 3.4660 - accuracy: 0.4437\nEpoch 173/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.4601 - accuracy: 0.4446\nEpoch 174/200\n273/273 [==============================] - 120s 438ms/step - loss: 3.4568 - accuracy: 0.4451\nEpoch 175/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.4526 - accuracy: 0.4452\nEpoch 176/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.4490 - accuracy: 0.4456\nEpoch 177/200\n273/273 [==============================] - 120s 439ms/step - loss: 3.4448 - accuracy: 0.4458\nEpoch 178/200\n273/273 [==============================] - 122s 445ms/step - loss: 3.4392 - accuracy: 0.4463\nEpoch 179/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.4360 - accuracy: 0.4467\nEpoch 180/200\n273/273 [==============================] - 121s 445ms/step - loss: 3.4312 - accuracy: 0.4469\nEpoch 181/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.4272 - accuracy: 0.4477\nEpoch 182/200\n273/273 [==============================] - 121s 441ms/step - loss: 3.4230 - accuracy: 0.4480\nEpoch 183/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.4195 - accuracy: 0.4483\nEpoch 184/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.4149 - accuracy: 0.4484\nEpoch 185/200\n273/273 [==============================] - 121s 442ms/step - loss: 3.4111 - accuracy: 0.4491\nEpoch 186/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.4079 - accuracy: 0.4494\nEpoch 187/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.4023 - accuracy: 0.4499\nEpoch 188/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.3985 - accuracy: 0.4502\nEpoch 189/200\n273/273 [==============================] - 121s 445ms/step - loss: 3.3948 - accuracy: 0.4505\nEpoch 190/200\n273/273 [==============================] - 122s 446ms/step - loss: 3.3909 - accuracy: 0.4505\nEpoch 191/200\n273/273 [==============================] - 121s 445ms/step - loss: 3.3875 - accuracy: 0.4510\nEpoch 192/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.3834 - accuracy: 0.4514\nEpoch 193/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.3783 - accuracy: 0.4519\nEpoch 194/200\n273/273 [==============================] - 121s 445ms/step - loss: 3.3734 - accuracy: 0.4527\nEpoch 195/200\n273/273 [==============================] - 122s 445ms/step - loss: 3.3702 - accuracy: 0.4525\nEpoch 196/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.3673 - accuracy: 0.4528\nEpoch 197/200\n273/273 [==============================] - 121s 443ms/step - loss: 3.3643 - accuracy: 0.4533\nEpoch 198/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.3604 - accuracy: 0.4537\nEpoch 199/200\n273/273 [==============================] - 121s 444ms/step - loss: 3.3558 - accuracy: 0.4539\nEpoch 200/200\n273/273 [==============================] - 120s 441ms/step - loss: 3.3514 - accuracy: 0.4544\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7db0ab82dd80>"},"metadata":{}}]},{"cell_type":"code","source":"def make_inference_models():\n    \n    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n    \n    decoder_state_input_h = tf.keras.layers.Input(shape=(None,))\n    decoder_state_input_c = tf.keras.layers.Input(shape=(None,))\n    \n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    \n    decoder_outputs, state_h, state_c = decoder_lstm(\n        decoder_embedding , initial_state=decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = decoder_dense(decoder_outputs)\n    decoder_model = tf.keras.models.Model(\n        [decoder_inputs] + decoder_states_inputs,\n        [decoder_outputs] + decoder_states)\n    \n    return encoder_model , decoder_model","metadata":{"execution":{"iopub.status.busy":"2023-08-03T03:55:08.262403Z","iopub.execute_input":"2023-08-03T03:55:08.262780Z","iopub.status.idle":"2023-08-03T03:55:08.270204Z","shell.execute_reply.started":"2023-08-03T03:55:08.262749Z","shell.execute_reply":"2023-08-03T03:55:08.269182Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ndef str_to_tokens( sentence : str ):\n    sentence = clean_text(sentence)\n    sentence = ' '.join([lemmatizer.lemmatize(word.lower(), pos=\"v\") for word in sentence \n                             if not word.lower() in stop_words and word not in punctuation])\n    words = sentence.split()\n    tokens_list = list()\n    for word in words:\n        tokens_list.append( questions_word_dict[ word ] ) \n    return pad_sequences( [tokens_list] , maxlen=max_question_length , padding='post')","metadata":{"execution":{"iopub.status.busy":"2023-08-03T04:12:09.713850Z","iopub.execute_input":"2023-08-03T04:12:09.714223Z","iopub.status.idle":"2023-08-03T04:12:09.721609Z","shell.execute_reply.started":"2023-08-03T04:12:09.714194Z","shell.execute_reply":"2023-08-03T04:12:09.720453Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"enc_model , dec_model = make_inference_models()\nfor epoch in range(8):\n    states_values = enc_model.predict( str_to_tokens( input( 'User: ' ) ) )\n    empty_target_seq = np.zeros( ( 1 , 1 ) )\n    empty_target_seq[0, 0] = answers_word_dict['start']\n    stop_condition = False\n    decoded_translation = ''\n    while not stop_condition :\n        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n        sampled_word = None\n        for word , index in answers_word_dict.items() :\n            if sampled_word_index == index :\n                decoded_translation += ' {}'.format( word )\n                sampled_word = word\n        \n        if sampled_word == 'end' or len(decoded_translation.split()) > max_answer_length:\n            stop_condition = True\n            \n        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        states_values = [ h , c ] \n\n    print( \"Bot:\" +decoded_translation.replace(' end', '') )\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T04:27:20.719025Z","iopub.execute_input":"2023-08-03T04:27:20.719416Z","iopub.status.idle":"2023-08-03T04:28:32.492478Z","shell.execute_reply.started":"2023-08-03T04:27:20.719383Z","shell.execute_reply":"2023-08-03T04:28:32.491431Z"},"trusted":true},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdin","text":"User:  hi\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 269ms/step\n1/1 [==============================] - 0s 298ms/step\n1/1 [==============================] - 0s 22ms/step\nBot: yeah\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  how are you?\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 23ms/step\nBot: oh right\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  You're ready for the quiz.\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nBot: right major\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  Looks like things worked out tonight, huh?\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\nBot: sure chisel\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  I was?\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\nBot: get\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  You never wanted to go out with 'me, did you?\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 28ms/step\nBot: yes sir\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  You always been this selfish?\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\nBot: yes sure\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"User:  What crap?\n"},{"name":"stdout","text":"1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 21ms/step\n1/1 [==============================] - 0s 22ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 21ms/step\nBot: five years ecological study northland oil\n\n","output_type":"stream"}]}]}